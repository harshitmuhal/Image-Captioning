{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import pickle,keras\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Flickr8k/Flickr_TextData/Flickr8k.token.txt') as f:\n",
    "    data=f.read()\n",
    "data=data.split('\\n')[:-1] #last entry is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "stemming ,lemmatising , stopword removal techniques have not been performed while cleaning the captions so as to teach model write captions with correct english otherwise - a caption like 'A child in a pink dress is climbing up a set of stairs in an entry way' model will predict caption of form 'child pink dress climb set of stairs entry way' which doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n",
    "    words=sentence.split()\n",
    "    words = [word for word in words if len(word)>1]\t\t\n",
    "    sentence =  ' '.join(words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions={}\n",
    "for i in range(len(data)):\n",
    "    img_name,img_caption=data[i].split('\\t')\n",
    "    img_name=img_name.split('.jpg')[0]\n",
    "    img_caption=clean_text(img_caption)\n",
    "    if captions.get(img_name) is None:\n",
    "        captions[img_name] = []\n",
    "    captions[img_name].append(img_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(captions)\n",
    "with open(\"cleaned_caption_description.json\", \"w\") as file:\n",
    "    json.dump(captions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_caption_description.json\", \"r\") as file:\n",
    "    captions=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=None\n",
    "test=None\n",
    "with open('../Flickr8k/Flickr_TextData/Flickr_8k.trainImages.txt') as f:\n",
    "    train=f.readlines()\n",
    "with open('../Flickr8k/Flickr_TextData/Flickr_8k.testImages.txt') as f:\n",
    "    test=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[c.split('.jpg')[0] for c in train]\n",
    "test=[c.split('.jpg')[0] for c in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2513260012_03d33305cf', '2903617548_d3e38d7f88', '3338291921_fe7ae0c8f8', '488416045_1c6d903fe0', '2644326817_8f45080b87']\n"
     ]
    }
   ],
   "source": [
    "print(train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<start\\> (start-of-sequence) and <end\\> (end-of-sequence) characters are added to every caption to help our model determine when to start and - more importantly - end sequences.\n",
    "Because while generating text(captions) model should know when it has to end otherwise it will keep predicting words and add them to the captions.  That's why we teach our model to decide the length of a caption on that by itself. when model will predict <end\\> as next word for the caption, model should stop predicting more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train descriptions\n",
    "train_descriptions={}\n",
    "for t in train:\n",
    "    train_descriptions[t]=[]\n",
    "    for sent in captions[t]:\n",
    "        new_caption='<start>'+sent+'<end>'\n",
    "        train_descriptions[t].append(new_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_descriptions.json','w') as file:\n",
    "    json.dump(train_descriptions,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_descriptions.json','r') as file:\n",
    "    train_descriptions=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Resnet model to extract features from image \n",
    "- Resnet is used as an encoder\n",
    "- output of 3rd last layer of resnet will be used as an encoding containing information about different features captured by resnet. These encodings will be later fed as input to the main model to predict captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights=\"imagenet\",input_shape=(224,224,3))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(model.input,model.layers[-2].output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_encodings(img):\n",
    "    feature_vector = new_model.predict(img)\n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = {}\n",
    "for img_name in train:\n",
    "    img_path='../Flickr8k/Images/'+img_name+'.jpg'\n",
    "    img = image.load_img(img_path,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1,224,224,3)) \n",
    "    #resnet expects a batch of image so expanding dimensions\n",
    "    # (224,224,3) -> (1,224,224,3)\n",
    "    img = preprocess_input(img)\n",
    "    encoding_train[img_name] = get_image_encodings(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_test = {}\n",
    "for img_name in test:\n",
    "    img_path='../Flickr8k/Images/'+img_name+'.jpg'\n",
    "    img = image.load_img(img_path,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1,224,224,3)) \n",
    "    img = preprocess_input(img)\n",
    "    encoding_test[img_name] = get_image_encodings(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_image_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_train,f)\n",
    "with open(\"encoded_image_features_testdata.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"encoded_image_features.pkl\",\"rb\") as f:\n",
    "    encoding_train=pickle.load(f)\n",
    "with open(\"encoded_image_features_testdata.pkl\",\"rb\") as f:\n",
    "    encoding_test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words= 351525\n",
      "no of unique words= 8522\n"
     ]
    }
   ],
   "source": [
    "total_words=[]\n",
    "for img_name in train_descriptions.keys():\n",
    "    [total_words.append(i) for sent in train_descriptions[img_name] for i in sent.split()]\n",
    "print('total words= {}'.format(len(total_words)))\n",
    "count=np.array(np.unique(total_words,return_counts=True))\n",
    "count=[(count[0][i],count[1][i])for i in range(count.shape[1])]\n",
    "print('no of unique words= {}'.format(len(count))) #no of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words= 1633\n"
     ]
    }
   ],
   "source": [
    "#reducing size of vocab by removing words with frequency less than 10\n",
    "size=10\n",
    "vocab=[w[0] for w in count if int(w[1])>size]\n",
    "print('no of unique words= {}'.format(len(vocab)))\n",
    "vocab_size=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words.append('<start>')\n",
    "total_words.append('<end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx={}\n",
    "idx_to_word={}\n",
    "# 0th index will be used to store character that will be used for padding while making \n",
    "# all captions of equal length (length of maximum sentence)\n",
    "# <start> dog (padding) (padding) (padding)\n",
    "# <start> dog is (padding) (padding)\n",
    "# <start> dog is playing \n",
    "for i,word in enumerate(vocab):\n",
    "    word_to_idx[word]=i+1\n",
    "    idx_to_word[i+1]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "max_len = 0 \n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        max_len = max(max_len,len(cap.split()))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "- Data is very big that loading a batch one by one in memory while training is adviced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGenerator(train_descriptions,batch_size,encoding_train,word_to_idx):\n",
    "    X1,X2,y=[],[],[]\n",
    "    #x1- image features, x2=captions ,y = next word that should be predicted\n",
    "    n =0\n",
    "    while True:\n",
    "        for key,desc_list in train_descriptions.items():\n",
    "            n += 1\n",
    "            photo = encoding_train[key]\n",
    "            for desc in desc_list:\n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    #0 denote padding word\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    yi = keras.utils.to_categorical([yi],num_classes=vocab_size)[0]\n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "                if n==batch_size:\n",
    "                    yield ([np.array(X1),np.array(X2)],np.array(y))\n",
    "                    X1,X2,y = [],[],[]\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_word_embeddings():\n",
    "    word_embeddings={}\n",
    "    with open('glove.6B.50d.txt',encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddings = np.asarray(values[1:],dtype='float32')\n",
    "            word_embeddings[word] = embeddings\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index=Get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding Matrix\n",
    "emb_dim = 50\n",
    "embedding_matrix = np.zeros((vocab_size,emb_dim))\n",
    "for word,idx in word_to_idx.items():\n",
    "    embedding_vector = embedding_index.get(word)    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_matrix.pkl','wb') as f:\n",
    "    pickle.dump(embedding_matrix,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embedding_matrix.pkl','rb') as f:\n",
    "    embedding_matrix=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch1 - for image features\n",
    "input_img_features = Input(shape=(2048,))\n",
    "inp_img1 = Dropout(0.3)(input_img_features)\n",
    "inp_img2 = Dense(256,activation='relu')(inp_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Branch2 - for Captions\n",
    "input_captions = Input(shape=(max_len,))\n",
    "inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\n",
    "inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "inp_cap3 = LSTM(256)(inp_cap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the branches\n",
    "decoder1 = add([inp_img2,inp_cap3])\n",
    "decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "outputs = Dense(vocab_size,activation='softmax')(decoder2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 37)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 2048)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 37, 50)       81700       input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 2048)         0           input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 37, 50)       0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 256)          524544      dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          314368      dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 256)          0           dense_12[0][0]                   \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 256)          65792       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1634)         419938      dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,406,342\n",
      "Trainable params: 1,406,342\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Final Model\n",
    "model = Model(inputs=[input_img_features,input_captions],outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Layer\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=optim,loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 673s 337ms/step - loss: 4.0343\n",
      "1519/2000 [=====================>........] - ETA: 2:49 - loss: 3.3725"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "number_pics_per_bath = 3\n",
    "steps = len(train_descriptions)//number_pics_per_bath\n",
    "for i in range(epochs):\n",
    "    generator = DataGenerator(train_descriptions,batch_size,encoding_train,word_to_idx)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_weights/model_one.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = DataGenerator(train_descriptions,batch_size,encoding_train,word_to_idx)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_weights/model_one.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.lr = 0.0001\n",
    "epochs = 5\n",
    "number_pics_per_bath = 6\n",
    "steps = len(train_descriptions)//number_pics_per_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_weights/model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./model_weights/model_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ./model_weights/model.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-e16efb5f2cc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model_weights/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./model_weights/model.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = load_model('./model_weigh ts/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_caption(photo):\n",
    "    \n",
    "    in_text = \"<start>\"\n",
    "    for i in range(max_len):\n",
    "        sequence = [word_to_idx[w] for w in in_text.split() if w in word_to_idx]\n",
    "        sequence = pad_sequences([sequence],maxlen=max_len,padding='post')\n",
    "        \n",
    "        ypred = model.predict([photo,sequence])\n",
    "        ypred = ypred.argmax() #WOrd with max prob always - Greedy Sampling\n",
    "        word = idx_to_word[ypred]\n",
    "        in_text += (' ' + word)\n",
    "        \n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "    \n",
    "    final_caption = in_text.split()[1:-1]\n",
    "    final_caption = ' '.join(final_caption)\n",
    "    return final_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick Some Random Images and See Results\n",
    "plt.style.use(\"seaborn\")\n",
    "for i in range(15):\n",
    "    idx = np.random.randint(0,1000)\n",
    "    all_img_names = list(encoding_test.keys())\n",
    "    img_name = all_img_names[idx]\n",
    "    photo_2048 = encoding_test[img_name].reshape((1,2048))\n",
    "    \n",
    "    i = plt.imread(\"../Flickr8k/Images/\"+img_name+\".jpg\")\n",
    "    \n",
    "    caption = predict_caption(photo_2048)\n",
    "    #print(caption)\n",
    "    \n",
    "    plt.title(caption)\n",
    "    plt.imshow(i)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
