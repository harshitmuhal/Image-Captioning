{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, LSTM\n",
    "from keras.layers.merge import add\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Flickr8k/Flickr_TextData/Flickr8k.token.txt') as f:\n",
    "    data=f.read()\n",
    "data=data.split('\\n')[:-1] #last entry is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "stemming ,lemmatising , stopword removal techniques have not been performed while cleaning the captions so as to teach model write captions with correct english otherwise - a caption like 'A child in a pink dress is climbing up a set of stairs in an entry way' model will predict caption of form 'child pink dress climb set of stairs entry way' which doesn't make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\"[^a-z]+\",\" \",sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions={}\n",
    "for i in range(len(data)):\n",
    "    img_name,img_caption=data[i].split('\\t')\n",
    "    img_name=img_name.split('.jpg')[0]\n",
    "    img_caption=clean_text(img_caption)\n",
    "    if captions.get(img_name) is None:\n",
    "        captions[img_name] = []\n",
    "    captions[img_name].append(img_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(captions)\n",
    "with open(\"cleaned_caption_description.json\", \"w\") as file:\n",
    "    json.dump(captions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cleaned_caption_description.json\", \"r\") as file:\n",
    "    captions=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=None\n",
    "test=[]\n",
    "with open('../Flickr8k/Flickr_TextData/Flickr_8k.trainImages.txt') as f:\n",
    "    train=f.readlines()\n",
    "with open('../Flickr8k/Flickr_TextData/Flickr_8k.testImages.txt') as f:\n",
    "    test=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[c.split('.jpg')[0] for c in train]\n",
    "test=[c.split('.jpg')[0] for c in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2513260012_03d33305cf', '2903617548_d3e38d7f88', '3338291921_fe7ae0c8f8', '488416045_1c6d903fe0', '2644326817_8f45080b87']\n"
     ]
    }
   ],
   "source": [
    "print(train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<start\\> (start-of-sequence) and <end\\> (end-of-sequence) characters are added to every caption to help our model determine when to start and - more importantly - end sequences.\n",
    "Because while generating text(captions) model should know when it has to end otherwise it will keep predicting words and add them to the captions.  That's why we teach our model to decide the length of a caption on that by itself. when model will predict <end\\> as next word for the caption, model should stop predicting more words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare train descriptions\n",
    "train_descriptions={}\n",
    "for t in train:\n",
    "    train_descriptions[t]=[]\n",
    "    for sent in captions[t]:\n",
    "        new_caption='<start>'+sent+'<end>'\n",
    "        train_descriptions[t].append(new_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_descriptions.json','w') as file:\n",
    "    json.dump(train_descriptions,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_descriptions.json','r') as file:\n",
    "    train_descriptions=json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Resnet model to extract features from image \n",
    "- Resnet is used as an encoder\n",
    "- output of 3rd last layer of resnet will be used as an encoding containing information about different features captured by resnet. These encodings will be later fed as input to the main model to predict captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights=\"imagenet\",input_shape=(224,224,3))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = Model(model.input,model.layers[-2].output)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_encodings(img):\n",
    "    feature_vector = new_model.predict(img)\n",
    "    feature_vector = feature_vector.reshape((-1,))\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_train = {}\n",
    "for img_name in train:\n",
    "    img_path='../Flickr8k/Images/'+img_name+'.jpg'\n",
    "    img = image.load_img(img_path,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1,224,224,3)) \n",
    "    #resnet expects a batch of image so expanding dimensions\n",
    "    # (224,224,3) -> (1,224,224,3)\n",
    "    img = preprocess_input(img)\n",
    "    encoding_train[img_name] = get_image_encodings(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_test = {}\n",
    "for img_name in test:\n",
    "    img_path='../Flickr8k/Images/'+img_name+'.jpg'\n",
    "    img = image.load_img(img_path,target_size=(224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img.reshape((1,224,224,3)) \n",
    "    img = preprocess_input(img)\n",
    "    encoding_test[img_name] = get_image_encodings(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"encoded_image_features.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_train,f)\n",
    "with open(\"encoded_image_features_testdata.pkl\",\"wb\") as f:\n",
    "    pickle.dump(encoding_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words= 437466\n",
      "no of unique words= 8441\n"
     ]
    }
   ],
   "source": [
    "total_words=[]\n",
    "for img_name in captions.keys():\n",
    "    [total_words.append(i) for sent in captions[img_name] for i in sent.split()]\n",
    "print('total words= {}'.format(len(total_words)))\n",
    "count=np.array(np.unique(total_words,return_counts=True))\n",
    "count=[(count[0][i],count[1][i])for i in range(count.shape[1])]\n",
    "print('no of unique words= {}'.format(len(count))) #no of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of unique words= 1850\n"
     ]
    }
   ],
   "source": [
    "#reducing size of vocab by removing words with frequency less than 10\n",
    "size=10\n",
    "vocab=[w[0] for w in count if int(w[1])>size]\n",
    "print('no of unique words= {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words.append('<start>')\n",
    "total_words.append('<end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx={}\n",
    "idx_to_word={}\n",
    "# 0th index will be used to store character that will be used for padding while making \n",
    "# all captions of equal length (length of maximum sentence)\n",
    "# <start> dog (padding) (padding) (padding)\n",
    "# <start> dog is (padding) (padding)\n",
    "# <start> dog is playing \n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_idx[word]=i+1\n",
    "    idx_to_word[i+1]=word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "max_len = 0 \n",
    "for key in train_descriptions.keys():\n",
    "    for cap in train_descriptions[key]:\n",
    "        max_len = max(max_len,len(cap.split()))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator\n",
    "- Data is very big that loading a batch one by one in memory while training is adviced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DataGenerator(train_descriptions,batch_size,encoding_train,word_to_idx):\n",
    "    X1,X2,Y=[],[],[]\n",
    "    #x1- image features, x2=captions ,y = next word that should be predicted\n",
    "    no=0\n",
    "    while True:\n",
    "        for img_name,caption_list in train_descriptions.items():\n",
    "            no+=1\n",
    "            img_encoding=encoding_train[img_name]\n",
    "            for sent in caption_list:\n",
    "                sent_=[word_to_idx[w] for w in sent if w in word_to_idx]\n",
    "                for i in range(len(sent_)):\n",
    "                    x=sent_[:i]\n",
    "                    y=sent_[i]\n",
    "                    x=pad_sequencesd([x],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    vocab_size=len(word_to_idx) + 1\n",
    "                    y=to_categorical([y],num_classes=vocab_size)[0]\n",
    "                    X1.append(img_encoding)\n",
    "                    X2.append(x)\n",
    "                    Y.append(y)\n",
    "            if(no==batch_size):\n",
    "                yield [[np.array(X1),np.array(X2)],np.array(Y)]\n",
    "                X1,X2,Y = [],[],[]\n",
    "                no = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
